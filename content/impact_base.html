<div class="description">
<p>Measuring the impact of data levers is a scientific process.</p>
<p>
  We can pose questions like: Given that some set of
  people take some action to add, withhold or alter some data records, how will
  affected AI systems change in terms of their capabilities?
</p>

<p>
  For instance, how will AI art capabilities change if a firm releases a model
  with no externally scraped data? Or, how will code-focused language model
  capabilities change after offering GitHub users an opt-out opportunity? 
</p>
  <p>For
  now, this page just includes a list of systems to watch. In the future, we'll
  provide summaries of how the capabilities of lever-impacted systems differ. For
  connections between measuring data lever impacts and scaling laws, see this
  <a href="https://observablehq.com/d/685789015c21cd9a">notebook</a>.
</p>
<ul>
  <li>
    Adobe's
    <a href="https://www.adobe.com/sensei/generative-ai/firefly.html">Firefly</a
    >. AI art with all licensed training data. 
    Can valuable insight into the value of art contributions.
  </li>
  <li>
    The <a href="https://www.bigcode-project.org/">BigCode</a> language model.
    AI code honoring data opt-out. Can provide insight into the value of code contributions.
  </li>
  <li>
    <a href="https://www.waterlily.ai/">Waterlily</a> "Ethical Generative AI-Art".
    Art model that pays artists whose style was used. Uses web3.
  </li>
  <li>More coming soon!</li>
</ul>

<p>
  This page is also under construction. We'll have more discussion of scaling
  laws and specific capability benchmarks soon!
</p>
</div>